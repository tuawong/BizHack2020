{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## Data Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=1',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=2',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=3',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=4',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=5',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=6',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=7',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=8',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=9',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=10',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=11',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=12',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=13',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=14',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=15',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=16',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=17',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=18',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=19',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=20',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=21',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=22',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=23',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=24',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=25',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=26',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=27',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=28',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=29',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=30',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=31',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=32',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=33',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=34',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=35',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=36',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=37',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=38',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=39',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=40',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=41',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=42',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=43',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=44',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=45',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=46',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=47',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=48',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=49',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=50',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=51',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=52',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=53',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=54',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=55',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=56',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=57',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=58',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=59',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=60',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=61',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=62',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=63',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=64',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=65',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=66',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=67',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=68',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=69',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=70',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=71',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=72',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=73',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=74',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=75',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=76',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=77',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=78',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=79',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=80',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=81',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=82',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=83',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=84',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=85',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=86',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=87',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=88',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=89',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=90',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=91',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=92',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=93',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=94',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=95',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=96',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=97',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=98',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=99',\n",
       " 'http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=100']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.DataFrame()\n",
    "urllist = [\"http://www.bestbuy.ca/api/v2/XML/search?categoryid=20352&pagesize=100&page=\" + str(i) for i in range(1,101)]\n",
    "urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "count=0 \n",
    "\n",
    "for url in urllist: \n",
    "    response_new = requests.get(url)\n",
    "    \n",
    "    if count == 0: \n",
    "        df = pd.DataFrame(xmltodict.parse(response_new.text)['SearchResponse']['Products']['Product'])\n",
    "        count += 1 \n",
    "    \n",
    "    else: \n",
    "        df = df.append(pd.DataFrame(xmltodict.parse(response_new.text)['SearchResponse']['Products']['Product']))\n",
    "\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Microcad Computer Corporation': 50,\n",
       " 'NetTradez': 528,\n",
       " 'Best Buy Business': 267,\n",
       " 'ORION COMPUTER AND ELECTRONICS': 12,\n",
       " 'PRO Open Box': 216,\n",
       " 'Olde Time Mac': 29,\n",
       " 'THE PC ROOM': 98,\n",
       " 'Electronics_Canada': 19,\n",
       " 'DCR': 102,\n",
       " 'Certified Refurbisher': 33,\n",
       " 'CDI Computer Dealers Inc.': 332,\n",
       " 'ALC Micro': 33,\n",
       " 'OEM': 37,\n",
       " 'Wireless Experts, Inc.': 2,\n",
       " 'Canada Wholesale': 10,\n",
       " 'AlwaysTech': 15,\n",
       " 'IGO Electronics': 85,\n",
       " 'Aero Malls Inc.': 22,\n",
       " 'DirectEASYBUY': 66,\n",
       " 'Mike’s Computer Shop': 342,\n",
       " 'TSC Deals': 19,\n",
       " 'Refurbanana': 3,\n",
       " 'Mega PC Mall': 2,\n",
       " 'OpenBox': 13,\n",
       " 'CPR upper beeches toronto': 2,\n",
       " 'RefurbIT': 39,\n",
       " 'SaveOnMacs': 19,\n",
       " 'MCI ONLINE': 37,\n",
       " 'MFG DIRECT ELECTRONICS': 9,\n",
       " 'TecnoCanada': 16,\n",
       " 'Canadian Outlet': 14,\n",
       " 'Tecnodeals': 103,\n",
       " 'Computer Depot': 41,\n",
       " 'QR Computers': 19,\n",
       " 'Advanced Skyline Technology Ltd.': 70,\n",
       " 'TVOUTLET': 12,\n",
       " 'Advanced Computers': 139,\n",
       " 'ACM Électronique': 18,\n",
       " 'Techdealz': 8,\n",
       " 'SIPL Online': 30,\n",
       " 'OneDealOutlet Canada': 175,\n",
       " 'Lenovo Canada': 69,\n",
       " 'Vancouver Electronic Depot': 10,\n",
       " 'Exooto Media': 13,\n",
       " 'Nem Distribution': 13,\n",
       " 'Square PC': 247,\n",
       " 'BigOnDeals': 12,\n",
       " 'Deal Targets': 277,\n",
       " 'VLCanada': 2,\n",
       " 'Newseed': 4,\n",
       " 'CELL4LESS': 5,\n",
       " 'X-Dynamic Systems': 17,\n",
       " 'iSanek': 124,\n",
       " 'MichaelElectronics2': 5208,\n",
       " 'TWI Store': 31,\n",
       " 'BidDeal': 23,\n",
       " 'Laptopking': 2,\n",
       " 'Etech': 3,\n",
       " 'TeleasyOnline': 26,\n",
       " 'COMPUPOINT INC.': 40,\n",
       " 'CTSMAR': 3,\n",
       " 'DealWiz': 20,\n",
       " 'Wintronic Computers Plus': 32,\n",
       " 'TechSavings': 29,\n",
       " 'Eurocom Corporation': 35,\n",
       " 'CFI eStore': 3,\n",
       " 'Experimax': 14,\n",
       " 'Vertex Nano Technology': 9,\n",
       " 'Outdoor Laptops': 22,\n",
       " 'Compute4Less Corp': 3,\n",
       " 'Canadian Electronics': 6,\n",
       " 'TECDATA ENGINEERS': 3,\n",
       " 'Best Tech Canada': 2,\n",
       " 'COMPACCESSORIES': 3,\n",
       " 'RUGGEDBOOKS Inc.': 26,\n",
       " 'Deals Next Door': 2,\n",
       " 'ITFactory': 3,\n",
       " 'Quick Tech': 3,\n",
       " 'TTWILI': 5,\n",
       " 'TechGenius': 16,\n",
       " 'Dr. Liquidator': 2,\n",
       " 'Fast Distribution': 2,\n",
       " 'Dealfactor Canada': 8,\n",
       " None: 2,\n",
       " 'UNIWAY': 5,\n",
       " 'EXPERTPRO': 12,\n",
       " 'Conquest Distributors': 2,\n",
       " 'Canada cellular': 7,\n",
       " 'Urban Inspirations': 2,\n",
       " 'YY Page': 2,\n",
       " '123wowdeals': 6,\n",
       " 'TiGuyCo Plus': 2,\n",
       " 'Sunterra Equipment Inc': 16,\n",
       " 'Datamatrix': 4,\n",
       " 'DigitalShopper': 8,\n",
       " 'ULTIMAXX CANADA': 4,\n",
       " 'CLX Gaming': 11,\n",
       " 'Micropeer': 2,\n",
       " 'Cellular Tech': 2,\n",
       " 'Techville': 2,\n",
       " 'GADGET HOUSE': 2,\n",
       " 'ERA': 3}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notBB = df[~df['Seller'].isnull()]\n",
    "\n",
    "seller = [notBB['Seller'].iloc[i].get('Name') for i in range(0,len(notBB))]\n",
    "sellunique =[] \n",
    "numsell = {} \n",
    "\n",
    "for x in seller: \n",
    "    if x not in sellunique: \n",
    "        sellunique.append(x)\n",
    "        numsell[x] = 1\n",
    "    if x in sellunique: \n",
    "        numsell[x] += 1 \n",
    "\n",
    "sellunique\n",
    "numsell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 131,\n",
       " 169,\n",
       " 337,\n",
       " 227,\n",
       " 351,\n",
       " 89,\n",
       " 185,\n",
       " 414,\n",
       " 118,\n",
       " 190,\n",
       " 282,\n",
       " 144,\n",
       " 324,\n",
       " 101,\n",
       " 101,\n",
       " 254,\n",
       " 244,\n",
       " 407,\n",
       " 133,\n",
       " 479,\n",
       " 222,\n",
       " 84,\n",
       " 346,\n",
       " 358,\n",
       " 150,\n",
       " 143,\n",
       " 173,\n",
       " 118,\n",
       " 66,\n",
       " 114,\n",
       " 112,\n",
       " 185,\n",
       " 467,\n",
       " 335,\n",
       " 362,\n",
       " 45,\n",
       " 48,\n",
       " 403,\n",
       " 205,\n",
       " 396,\n",
       " 479,\n",
       " 87,\n",
       " 94,\n",
       " 389,\n",
       " 388,\n",
       " 83,\n",
       " 107,\n",
       " 297,\n",
       " 496,\n",
       " 157,\n",
       " 132,\n",
       " 202,\n",
       " 104,\n",
       " 348,\n",
       " 277,\n",
       " 356,\n",
       " 363,\n",
       " 102,\n",
       " 399,\n",
       " 140,\n",
       " 381,\n",
       " 479,\n",
       " 368,\n",
       " 199,\n",
       " 125,\n",
       " 87,\n",
       " 230,\n",
       " 183,\n",
       " 260,\n",
       " 383,\n",
       " 479,\n",
       " 494,\n",
       " 307,\n",
       " 144,\n",
       " 345,\n",
       " 160,\n",
       " 411,\n",
       " 148,\n",
       " 481,\n",
       " 131,\n",
       " 249,\n",
       " 332,\n",
       " 349,\n",
       " 443,\n",
       " 479,\n",
       " 101,\n",
       " 481,\n",
       " 207,\n",
       " 221,\n",
       " 132,\n",
       " 153,\n",
       " 481,\n",
       " 363,\n",
       " 121,\n",
       " 131,\n",
       " 215,\n",
       " 220,\n",
       " 120,\n",
       " 169,\n",
       " 291,\n",
       " 415,\n",
       " 326,\n",
       " 205,\n",
       " 353,\n",
       " 88,\n",
       " 187,\n",
       " 107,\n",
       " 86,\n",
       " 132,\n",
       " 465,\n",
       " 129,\n",
       " 214,\n",
       " 359,\n",
       " 354,\n",
       " 354,\n",
       " 246,\n",
       " 130,\n",
       " 354,\n",
       " 144,\n",
       " 341,\n",
       " 82,\n",
       " 340,\n",
       " 286,\n",
       " 320,\n",
       " 275,\n",
       " 140,\n",
       " 375,\n",
       " 76,\n",
       " 353,\n",
       " 412,\n",
       " 465,\n",
       " 119,\n",
       " 126,\n",
       " 345,\n",
       " 358,\n",
       " 110,\n",
       " 383,\n",
       " 111,\n",
       " 298,\n",
       " 278,\n",
       " 412,\n",
       " 259,\n",
       " 409,\n",
       " 174,\n",
       " 479,\n",
       " 303,\n",
       " 345,\n",
       " 279,\n",
       " 370,\n",
       " 110,\n",
       " 479,\n",
       " 94,\n",
       " 154,\n",
       " 87,\n",
       " 92,\n",
       " 317,\n",
       " 66,\n",
       " 321,\n",
       " 344,\n",
       " 83,\n",
       " 313,\n",
       " 432,\n",
       " 479,\n",
       " 127,\n",
       " 143,\n",
       " 363,\n",
       " 479,\n",
       " 101,\n",
       " 479,\n",
       " 352,\n",
       " 279,\n",
       " 188,\n",
       " 258,\n",
       " 339,\n",
       " 294,\n",
       " 107,\n",
       " 479,\n",
       " 481,\n",
       " 481,\n",
       " 225,\n",
       " 343,\n",
       " 247,\n",
       " 250,\n",
       " 235,\n",
       " 270,\n",
       " 331,\n",
       " 362,\n",
       " 160,\n",
       " 71,\n",
       " 31,\n",
       " 102,\n",
       " 117,\n",
       " 82,\n",
       " 57,\n",
       " 352,\n",
       " 300,\n",
       " 371,\n",
       " 368,\n",
       " 386,\n",
       " 66,\n",
       " 383,\n",
       " 354,\n",
       " 283,\n",
       " 123,\n",
       " 117,\n",
       " 317,\n",
       " 416,\n",
       " 479,\n",
       " 158,\n",
       " 198,\n",
       " 155,\n",
       " 166,\n",
       " 326,\n",
       " 340,\n",
       " 248,\n",
       " 190,\n",
       " 99,\n",
       " 358,\n",
       " 336,\n",
       " 479,\n",
       " 120,\n",
       " 364,\n",
       " 339,\n",
       " 427,\n",
       " 369,\n",
       " 349,\n",
       " 255,\n",
       " 122,\n",
       " 324,\n",
       " 276,\n",
       " 211,\n",
       " 379,\n",
       " 337,\n",
       " 330,\n",
       " 238,\n",
       " 124,\n",
       " 146,\n",
       " 87,\n",
       " 372,\n",
       " 372,\n",
       " 104,\n",
       " 208,\n",
       " 234,\n",
       " 302,\n",
       " 349,\n",
       " 364,\n",
       " 159,\n",
       " 210,\n",
       " 84,\n",
       " 325,\n",
       " 120,\n",
       " 369,\n",
       " 169,\n",
       " 379,\n",
       " 282,\n",
       " 322,\n",
       " 332,\n",
       " 130,\n",
       " 175,\n",
       " 347,\n",
       " 489,\n",
       " 496,\n",
       " 341,\n",
       " 36,\n",
       " 133,\n",
       " 105,\n",
       " 336,\n",
       " 337,\n",
       " 479,\n",
       " 332,\n",
       " 478,\n",
       " 356,\n",
       " 300,\n",
       " 453,\n",
       " 339,\n",
       " 344,\n",
       " 338,\n",
       " 358,\n",
       " 231,\n",
       " 339,\n",
       " 479,\n",
       " 326,\n",
       " 214,\n",
       " 395,\n",
       " 130,\n",
       " 370,\n",
       " 75,\n",
       " 335,\n",
       " 479,\n",
       " 262,\n",
       " 351,\n",
       " 107,\n",
       " 107,\n",
       " 121,\n",
       " 330,\n",
       " 385,\n",
       " 479,\n",
       " 475,\n",
       " 349,\n",
       " 462,\n",
       " 208,\n",
       " 345,\n",
       " 372,\n",
       " 340,\n",
       " 157,\n",
       " 312,\n",
       " 354,\n",
       " 261,\n",
       " 195,\n",
       " 79,\n",
       " 371,\n",
       " 332,\n",
       " 184,\n",
       " 164,\n",
       " 164,\n",
       " 302,\n",
       " 176,\n",
       " 368,\n",
       " 68,\n",
       " 464,\n",
       " 368,\n",
       " 333,\n",
       " 369,\n",
       " 396,\n",
       " 199,\n",
       " 278,\n",
       " 83,\n",
       " 336,\n",
       " 339,\n",
       " 195,\n",
       " 481,\n",
       " 371,\n",
       " 249,\n",
       " 113,\n",
       " 247,\n",
       " 390,\n",
       " 140,\n",
       " 282,\n",
       " 479,\n",
       " 92,\n",
       " 263,\n",
       " 90,\n",
       " 352,\n",
       " 203,\n",
       " 382,\n",
       " 295,\n",
       " 103,\n",
       " 350,\n",
       " 349,\n",
       " 185,\n",
       " 131,\n",
       " 340,\n",
       " 290,\n",
       " 479,\n",
       " 272,\n",
       " 193,\n",
       " 126,\n",
       " 118,\n",
       " 347,\n",
       " 176,\n",
       " 479,\n",
       " 389,\n",
       " 103,\n",
       " 177,\n",
       " 98,\n",
       " 429,\n",
       " 479,\n",
       " 284,\n",
       " 122,\n",
       " 283,\n",
       " 212,\n",
       " 479,\n",
       " 250,\n",
       " 267,\n",
       " 197,\n",
       " 361,\n",
       " 185,\n",
       " 81,\n",
       " 92,\n",
       " 362,\n",
       " 369,\n",
       " 124,\n",
       " 316,\n",
       " 359,\n",
       " 347,\n",
       " 159,\n",
       " 367,\n",
       " 248,\n",
       " 153,\n",
       " 115,\n",
       " 481,\n",
       " 395,\n",
       " 315,\n",
       " 200,\n",
       " 127,\n",
       " 330,\n",
       " 250,\n",
       " 138,\n",
       " 328,\n",
       " 275,\n",
       " 346,\n",
       " 321,\n",
       " 276,\n",
       " 247,\n",
       " 487,\n",
       " 119,\n",
       " 499,\n",
       " 207,\n",
       " 464,\n",
       " 149,\n",
       " 356,\n",
       " 280,\n",
       " 479,\n",
       " 471,\n",
       " 479,\n",
       " 81,\n",
       " 449,\n",
       " 92,\n",
       " 232,\n",
       " 259,\n",
       " 149,\n",
       " 120,\n",
       " 157,\n",
       " 131,\n",
       " 479,\n",
       " 125,\n",
       " 479,\n",
       " 230,\n",
       " 112,\n",
       " 92,\n",
       " 301,\n",
       " 128,\n",
       " 159,\n",
       " 392,\n",
       " 459,\n",
       " 263,\n",
       " 326,\n",
       " 157,\n",
       " 152,\n",
       " 124,\n",
       " 200,\n",
       " 340,\n",
       " 27,\n",
       " 201,\n",
       " 318,\n",
       " 289,\n",
       " 35,\n",
       " 341,\n",
       " 170,\n",
       " 338,\n",
       " 300,\n",
       " 247,\n",
       " 152,\n",
       " 154,\n",
       " 120,\n",
       " 281,\n",
       " 138,\n",
       " 264,\n",
       " 233,\n",
       " 340,\n",
       " 481,\n",
       " 382,\n",
       " 109,\n",
       " 334,\n",
       " 139,\n",
       " 157,\n",
       " 121,\n",
       " 63,\n",
       " 153,\n",
       " 93,\n",
       " 100,\n",
       " 121,\n",
       " 145,\n",
       " 99,\n",
       " 158,\n",
       " 126,\n",
       " 338,\n",
       " 161,\n",
       " 372,\n",
       " 122,\n",
       " 122,\n",
       " 495,\n",
       " 343,\n",
       " 170,\n",
       " 148,\n",
       " 419,\n",
       " 128,\n",
       " 305,\n",
       " 187,\n",
       " 329,\n",
       " 479,\n",
       " 307,\n",
       " 307,\n",
       " 99,\n",
       " 351,\n",
       " 124,\n",
       " 200,\n",
       " 225,\n",
       " 257,\n",
       " 345,\n",
       " 313,\n",
       " 368,\n",
       " 90,\n",
       " 153,\n",
       " 158,\n",
       " 100,\n",
       " 101,\n",
       " 208,\n",
       " 387,\n",
       " 93,\n",
       " 171,\n",
       " 294,\n",
       " 278,\n",
       " 382,\n",
       " 125,\n",
       " 292,\n",
       " 158,\n",
       " 99,\n",
       " 342,\n",
       " 277,\n",
       " 240,\n",
       " 479,\n",
       " 244,\n",
       " 247,\n",
       " 117,\n",
       " 349,\n",
       " 478,\n",
       " 382,\n",
       " 264,\n",
       " 341,\n",
       " 441,\n",
       " 125,\n",
       " 330,\n",
       " 255,\n",
       " 451,\n",
       " 232,\n",
       " 80,\n",
       " 143,\n",
       " 161,\n",
       " 345,\n",
       " 165,\n",
       " 121,\n",
       " 125,\n",
       " 116,\n",
       " 156,\n",
       " 125,\n",
       " 131,\n",
       " 382,\n",
       " 121,\n",
       " 92,\n",
       " 278,\n",
       " 105,\n",
       " 80,\n",
       " 396,\n",
       " 209,\n",
       " 320,\n",
       " 115,\n",
       " 130,\n",
       " 140,\n",
       " 239,\n",
       " 363,\n",
       " 160,\n",
       " 271,\n",
       " 341,\n",
       " 106,\n",
       " 252,\n",
       " 257,\n",
       " 481,\n",
       " 352,\n",
       " 293,\n",
       " 102,\n",
       " 128,\n",
       " 247,\n",
       " 479,\n",
       " 346,\n",
       " 462,\n",
       " 105,\n",
       " 116,\n",
       " 340,\n",
       " 363,\n",
       " 327,\n",
       " 126,\n",
       " 467,\n",
       " 155,\n",
       " 244,\n",
       " 87,\n",
       " 341,\n",
       " 108,\n",
       " 106,\n",
       " 129,\n",
       " 361,\n",
       " 370,\n",
       " 84,\n",
       " 86,\n",
       " 196,\n",
       " 115,\n",
       " 96,\n",
       " 479,\n",
       " 382,\n",
       " 479,\n",
       " 263,\n",
       " 123,\n",
       " 88,\n",
       " 72,\n",
       " 399,\n",
       " 357,\n",
       " 428,\n",
       " 477,\n",
       " 104,\n",
       " 157,\n",
       " 238,\n",
       " 149,\n",
       " 99,\n",
       " 306,\n",
       " 198,\n",
       " 129,\n",
       " 92,\n",
       " 274,\n",
       " 249,\n",
       " 92,\n",
       " 169,\n",
       " 124,\n",
       " 274,\n",
       " 313,\n",
       " 124,\n",
       " 157,\n",
       " 71,\n",
       " 358,\n",
       " 104,\n",
       " 148,\n",
       " 120,\n",
       " 496,\n",
       " 483,\n",
       " 119,\n",
       " 479,\n",
       " 324,\n",
       " 84,\n",
       " 118,\n",
       " 117,\n",
       " 92,\n",
       " 163,\n",
       " 310,\n",
       " 83,\n",
       " 382,\n",
       " 496,\n",
       " 321,\n",
       " 340,\n",
       " 158,\n",
       " 94,\n",
       " 211,\n",
       " 157,\n",
       " 479,\n",
       " 257,\n",
       " 232,\n",
       " 220,\n",
       " 208,\n",
       " 212,\n",
       " 495,\n",
       " 168,\n",
       " 123,\n",
       " 388,\n",
       " 312,\n",
       " 247,\n",
       " 212,\n",
       " 331,\n",
       " 266,\n",
       " 132,\n",
       " 338,\n",
       " 148,\n",
       " 322,\n",
       " 163,\n",
       " 312,\n",
       " 302,\n",
       " 72,\n",
       " 343,\n",
       " 107,\n",
       " 490,\n",
       " 119,\n",
       " 124,\n",
       " 396,\n",
       " 125,\n",
       " 354,\n",
       " 112,\n",
       " 125,\n",
       " 118,\n",
       " 330,\n",
       " 131,\n",
       " 114,\n",
       " 90,\n",
       " 355,\n",
       " 471,\n",
       " 125,\n",
       " 78,\n",
       " 304,\n",
       " 209,\n",
       " 431,\n",
       " 99,\n",
       " 422,\n",
       " 179,\n",
       " 218,\n",
       " 261,\n",
       " 462,\n",
       " 126,\n",
       " 219,\n",
       " 191,\n",
       " 261,\n",
       " 141,\n",
       " 252,\n",
       " 347,\n",
       " 91,\n",
       " 389,\n",
       " 478,\n",
       " 493,\n",
       " 118,\n",
       " 350,\n",
       " 124,\n",
       " 492,\n",
       " 479,\n",
       " 152,\n",
       " 132,\n",
       " 208,\n",
       " 300,\n",
       " 348,\n",
       " 242,\n",
       " 444,\n",
       " 90,\n",
       " 479,\n",
       " 114,\n",
       " 82,\n",
       " 206,\n",
       " 261,\n",
       " 229,\n",
       " 265,\n",
       " 479,\n",
       " 131,\n",
       " 104,\n",
       " 342,\n",
       " 312,\n",
       " 479,\n",
       " 83,\n",
       " 107,\n",
       " 400,\n",
       " 66,\n",
       " 303,\n",
       " 82,\n",
       " 276,\n",
       " 287,\n",
       " 145,\n",
       " 125,\n",
       " 222,\n",
       " 472,\n",
       " 131,\n",
       " 160,\n",
       " 451,\n",
       " 105,\n",
       " 122,\n",
       " 35,\n",
       " 149,\n",
       " 213,\n",
       " 129,\n",
       " 479,\n",
       " 100,\n",
       " 479,\n",
       " 347,\n",
       " 231,\n",
       " 392,\n",
       " 332,\n",
       " 479,\n",
       " 187,\n",
       " 295,\n",
       " 488,\n",
       " 253,\n",
       " 373,\n",
       " 182,\n",
       " 428,\n",
       " 332,\n",
       " 243,\n",
       " 269,\n",
       " 456,\n",
       " 135,\n",
       " 327,\n",
       " 122,\n",
       " 119,\n",
       " 479,\n",
       " 485,\n",
       " 357,\n",
       " 245,\n",
       " 205,\n",
       " 382,\n",
       " 118,\n",
       " 429,\n",
       " 198,\n",
       " 346,\n",
       " 126,\n",
       " 100,\n",
       " 345,\n",
       " 94,\n",
       " 481,\n",
       " 301,\n",
       " 130,\n",
       " 215,\n",
       " 337,\n",
       " 118,\n",
       " 336,\n",
       " 263,\n",
       " 250,\n",
       " 302,\n",
       " 123,\n",
       " 125,\n",
       " 399,\n",
       " 126,\n",
       " 443,\n",
       " 338,\n",
       " 127,\n",
       " 112,\n",
       " 336,\n",
       " 209,\n",
       " 500,\n",
       " 82,\n",
       " 222,\n",
       " 122,\n",
       " 372,\n",
       " 90,\n",
       " 226,\n",
       " 351,\n",
       " 257,\n",
       " 321,\n",
       " 268,\n",
       " 500,\n",
       " 323,\n",
       " 319,\n",
       " 215,\n",
       " 327,\n",
       " 300,\n",
       " 479,\n",
       " 246,\n",
       " 496,\n",
       " 479,\n",
       " 135,\n",
       " 401,\n",
       " 247,\n",
       " 383,\n",
       " 183,\n",
       " 205,\n",
       " 270,\n",
       " 376,\n",
       " 386,\n",
       " 226,\n",
       " 171,\n",
       " 102,\n",
       " 302,\n",
       " 134,\n",
       " 370,\n",
       " 267,\n",
       " 447,\n",
       " 145,\n",
       " 106,\n",
       " 253,\n",
       " 404,\n",
       " 137,\n",
       " 246,\n",
       " 444,\n",
       " 73,\n",
       " 390,\n",
       " 479,\n",
       " 208,\n",
       " 287,\n",
       " 124,\n",
       " 223,\n",
       " 125,\n",
       " 483,\n",
       " 140,\n",
       " 327,\n",
       " 257,\n",
       " 221,\n",
       " 302,\n",
       " 117,\n",
       " 145,\n",
       " 66,\n",
       " 268,\n",
       " 488,\n",
       " 153,\n",
       " 496,\n",
       " 72,\n",
       " 109,\n",
       " 351,\n",
       " 372,\n",
       " 92,\n",
       " 307,\n",
       " 496,\n",
       " 124,\n",
       " 312,\n",
       " 323,\n",
       " 298,\n",
       " 367,\n",
       " 108,\n",
       " 151,\n",
       " 124,\n",
       " 98,\n",
       " 173,\n",
       " 250,\n",
       " 351,\n",
       " 247,\n",
       " 123,\n",
       " 364,\n",
       " 130,\n",
       " 464,\n",
       " 56,\n",
       " 93,\n",
       " 94,\n",
       " 91,\n",
       " 267,\n",
       " 224,\n",
       " 89,\n",
       " 99,\n",
       " 378,\n",
       " 340,\n",
       " 467,\n",
       " 110,\n",
       " 80,\n",
       " 197,\n",
       " 379,\n",
       " 116,\n",
       " 165,\n",
       " 481,\n",
       " 331,\n",
       " 95,\n",
       " 343,\n",
       " 109,\n",
       " 312,\n",
       " 100,\n",
       " 120,\n",
       " 433,\n",
       " 140,\n",
       " 176,\n",
       " 307,\n",
       " 330,\n",
       " 189,\n",
       " 112,\n",
       " 257,\n",
       " 150,\n",
       " 237,\n",
       " 190,\n",
       " 346,\n",
       " 388,\n",
       " 482,\n",
       " 395,\n",
       " 246,\n",
       " 469,\n",
       " 113,\n",
       " 318,\n",
       " 174,\n",
       " 80,\n",
       " 167,\n",
       " 109,\n",
       " 481,\n",
       " 383,\n",
       " 388,\n",
       " 94,\n",
       " 108,\n",
       " 79,\n",
       " 291,\n",
       " 83,\n",
       " 212,\n",
       " 234,\n",
       " 148,\n",
       " 109,\n",
       " 263,\n",
       " 99,\n",
       " 232,\n",
       " 461,\n",
       " 479,\n",
       " 351,\n",
       " 243,\n",
       " 186,\n",
       " 115,\n",
       " 125,\n",
       " 106,\n",
       " 239,\n",
       " 245,\n",
       " 135,\n",
       " 485,\n",
       " 92,\n",
       " 30,\n",
       " 247,\n",
       " 122,\n",
       " 328,\n",
       " 147,\n",
       " 446,\n",
       " 310,\n",
       " 230,\n",
       " 124,\n",
       " 122,\n",
       " 62,\n",
       " 349,\n",
       " 206,\n",
       " 401,\n",
       " 392,\n",
       " 103,\n",
       " ...]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desclen = [len(notBB['ShortDescription'].iloc[i]) for i in range(0,len(notBB))]\n",
    "desclen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "notBB['sellername'] = seller\n",
    "notBB['length'] = desclen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |  \n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be a sequence of items that\n",
      " |      can be of type string or byte.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : boolean, True by default\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_: boolean\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> print(vectorizer2.get_feature_names())\n",
      " |  ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |  'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |   'this document', 'this is', 'this the']\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list\n",
      " |          A list of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing, tokenization\n",
      " |      and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "shortdesc = notBB[['ShortDescription']]\n",
    "wordcount = cv.fit_transform(shortdesc)\n",
    "wordcount.toarray()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
